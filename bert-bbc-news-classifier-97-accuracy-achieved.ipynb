{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-03T19:50:17.492136Z","iopub.execute_input":"2026-01-03T19:50:17.492410Z","iopub.status.idle":"2026-01-03T19:50:17.830778Z","shell.execute_reply.started":"2026-01-03T19:50:17.492387Z","shell.execute_reply":"2026-01-03T19:50:17.830057Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/bbc-articles-dataset/bbc_news_text_complexity_summarization.csv\n/kaggle/input/bbc-articles-dataset/archive (2)/bbc-news-data.csv\n/kaggle/input/bbc-articles-dataset/archive/bbc_text_cls.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Multi-Class News Classification: Fine-Tuning BERT for BBC Content Analysis","metadata":{}},{"cell_type":"markdown","source":"### Import Libraries","metadata":{}},{"cell_type":"code","source":"# 1. Update pyarrow first to fix binary incompatibility, then install others\n!pip install -q -U pyarrow datasets transformers accelerate evaluate\n!pip install -q evaluate\nimport pandas as pd\nimport torch\nfrom datasets import Dataset\nfrom transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\nimport pyarrow # Verifying the fix\nimport evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T19:53:53.910929Z","iopub.execute_input":"2026-01-03T19:53:53.911607Z","iopub.status.idle":"2026-01-03T19:54:01.815903Z","shell.execute_reply.started":"2026-01-03T19:53:53.911582Z","shell.execute_reply":"2026-01-03T19:54:01.814956Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### load Data","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = pd.read_csv(\"/kaggle/input/bbc-articles-dataset/bbc_news_text_complexity_summarization.csv\")\ndf.columns = df.columns.str.strip()\n\n# Set column names based on your file\ntarget_col = 'labels' \ntext_col = 'text'\n\n# Convert text categories to numerical IDs\ndf[target_col] = df[target_col].astype('category')\nlabel_map = dict(enumerate(df[target_col].cat.categories))\ndf['labels'] = df[target_col].cat.codes  # Plural 'labels' for BERT loss calculation\n\nnum_labels = len(label_map)\nprint(f\"Detected {num_labels} classes: {label_map}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T19:54:13.561363Z","iopub.execute_input":"2026-01-03T19:54:13.561661Z","iopub.status.idle":"2026-01-03T19:54:13.651902Z","shell.execute_reply.started":"2026-01-03T19:54:13.561633Z","shell.execute_reply":"2026-01-03T19:54:13.651219Z"}},"outputs":[{"name":"stdout","text":"Detected 5 classes: {0: 'business', 1: 'entertainment', 2: 'politics', 3: 'sport', 4: 'tech'}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"### Data Tokenization and Dataset Preparation","metadata":{}},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[text_col], \n        padding=\"max_length\", \n        truncation=True, \n        max_length=256\n    )\n\n# Convert to Hugging Face Dataset and Split\nhf_dataset = Dataset.from_pandas(df[[text_col, 'labels']])\nhf_dataset = hf_dataset.train_test_split(test_size=0.2, seed=42)\n\n# Apply tokenization and format for PyTorch\nencoded_dataset = hf_dataset.map(tokenize_function, batched=True)\nencoded_dataset = encoded_dataset.remove_columns([text_col])\nencoded_dataset.set_format(\"torch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T19:54:16.404989Z","iopub.execute_input":"2026-01-03T19:54:16.405286Z","iopub.status.idle":"2026-01-03T19:54:30.748861Z","shell.execute_reply.started":"2026-01-03T19:54:16.405263Z","shell.execute_reply":"2026-01-03T19:54:30.748216Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1701 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397154063d684d3dad80685ee55ac1dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/426 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad332a375f04fd1958a9840677c14cd"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"### Model Initialization and Training Configuration","metadata":{}},{"cell_type":"code","source":"# Load Model\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\", \n    num_labels=num_labels\n)\n\n# Load Metric\nmetric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=predictions, references=labels)\n\n# Configure Training\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",      # Updated from evaluation_strategy\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    logging_steps=10,           # Log more frequently to see training loss\n    report_to=\"none\"\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_dataset[\"train\"],\n    eval_dataset=encoded_dataset[\"test\"],\n    compute_metrics=compute_metrics\n)\n# Start Training\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T19:54:33.615314Z","iopub.execute_input":"2026-01-03T19:54:33.615643Z","iopub.status.idle":"2026-01-03T19:56:57.009552Z","shell.execute_reply.started":"2026-01-03T19:54:33.615618Z","shell.execute_reply":"2026-01-03T19:56:57.008850Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84ef604aeb184d9889f971511a67ad76"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='162' max='162' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [162/162 02:18, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.430000</td>\n      <td>0.303737</td>\n      <td>0.971831</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.117200</td>\n      <td>0.114240</td>\n      <td>0.978873</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.060400</td>\n      <td>0.094417</td>\n      <td>0.978873</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=162, training_loss=0.3877436544424222, metrics={'train_runtime': 141.5347, 'train_samples_per_second': 36.055, 'train_steps_per_second': 1.145, 'total_flos': 671345940496896.0, 'train_loss': 0.3877436544424222, 'epoch': 3.0})"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"### Test","metadata":{}},{"cell_type":"code","source":"# 1. Define your custom text\ntext = \"The team won the championship after a spectacular goal in the final minute!\"\n\n# 2. Tokenize and move to the same device as the model\ninputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n\n# 3. Get prediction\nmodel.eval()\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# 4. Find the category index\nprediction_id = torch.argmax(outputs.logits, dim=1).item()\n\n# 5. Print the actual category name using your label_map\nprint(f\"Predicted Category: {label_map[prediction_id]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-03T19:57:03.512091Z","iopub.execute_input":"2026-01-03T19:57:03.512798Z","iopub.status.idle":"2026-01-03T19:57:03.561963Z","shell.execute_reply.started":"2026-01-03T19:57:03.512769Z","shell.execute_reply":"2026-01-03T19:57:03.561286Z"}},"outputs":[{"name":"stdout","text":"Predicted Category: sport\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}